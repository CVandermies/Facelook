{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facelook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk1yNDFs4bF_"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization, Conv2D, MaxPooling2D\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adadelta, Adam, Adamax, Adagrad, RMSprop, SGD\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "amount_classes=7\n",
        "img_rows,img_cols=48,48\n",
        "batch_size=32\n",
        "!wget https://www.dropbox.com/s/n8bninud94blch4/fer2013.csv\n",
        "fer_data = pd.read_csv(\"fer2013.csv\")\n",
        "\n",
        "# source https://www.kaggle.com/davidvictor/face-classification/notebook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ea2pGziuSjH",
        "outputId": "4ae10462-4f71-41b9-df76-9855ffa38f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-30 11:32:07--  https://www.dropbox.com/s/n8bninud94blch4/fer2013.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6021:18::a27d:4112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/n8bninud94blch4/fer2013.csv [following]\n",
            "--2021-12-30 11:32:07--  https://www.dropbox.com/s/raw/n8bninud94blch4/fer2013.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb8a8fd9720fdab4ec80eabf1db.dl.dropboxusercontent.com/cd/0/inline/Bc3T_hzm0U3VytWF0AUx3ZzAc2FHYWA0yKr-WSejyJblN6qet_jO-YZquqrwWyZokbmFOa8OVUJ2fO9oZtQHJUXYXSClK9L7WkzSVS6TcwNpUJ22lNS46WKC_OaZPGZBHrCgEGOO-484Pxaxw8JNq7UM/file# [following]\n",
            "--2021-12-30 11:32:07--  https://ucb8a8fd9720fdab4ec80eabf1db.dl.dropboxusercontent.com/cd/0/inline/Bc3T_hzm0U3VytWF0AUx3ZzAc2FHYWA0yKr-WSejyJblN6qet_jO-YZquqrwWyZokbmFOa8OVUJ2fO9oZtQHJUXYXSClK9L7WkzSVS6TcwNpUJ22lNS46WKC_OaZPGZBHrCgEGOO-484Pxaxw8JNq7UM/file\n",
            "Resolving ucb8a8fd9720fdab4ec80eabf1db.dl.dropboxusercontent.com (ucb8a8fd9720fdab4ec80eabf1db.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6026:15::a27d:460f\n",
            "Connecting to ucb8a8fd9720fdab4ec80eabf1db.dl.dropboxusercontent.com (ucb8a8fd9720fdab4ec80eabf1db.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 301072766 (287M) [text/plain]\n",
            "Saving to: ‘fer2013.csv’\n",
            "\n",
            "fer2013.csv         100%[===================>] 287.12M  21.3MB/s    in 13s     \n",
            "\n",
            "2021-12-30 11:32:22 (21.4 MB/s) - ‘fer2013.csv’ saved [301072766/301072766]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# dataset-format : x -> image matrix, y -> label\n",
        "\n",
        "# emotion_data = pd.read_csv('./data/fer2013.csv')\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = []\n",
        "for index, row in fer_data.iterrows():\n",
        "    k = row['pixels'].split(\" \")\n",
        "    if row['Usage'] == 'Training':\n",
        "        x_train.append(np.array(k))\n",
        "        y_train.append(row['emotion'])\n",
        "    elif row['Usage'] == 'PublicTest':\n",
        "        x_test.append(np.array(k))\n",
        "        y_test.append(row['emotion'])\n",
        "\n",
        "print(len(x_train))\n",
        "print(len(x_test))\n",
        "\n",
        "# source : https://www.analyticsvidhya.com/blog/2021/11/facial-emotion-detection-using-cnn/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38hEIH7ZOEmW",
        "outputId": "8848fa9e-4ff1-4a72-88ae-f089bf85d8d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28709\n",
            "3589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the List object to nparray\n",
        "\n",
        "x_train = np.array(x_train, dtype='float64')\n",
        "y_train = np.array(y_train, dtype='float64')\n",
        "x_test = np.array(x_test, dtype='float64')\n",
        "y_test = np.array(y_test, dtype='float64')\n",
        "\n",
        "# Format the array to a 48*48 matrix\n",
        "x_train = x_train.reshape(x_train.shape[0], 48, 48, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 48, 48, 1)"
      ],
      "metadata": {
        "id": "_m9LucEtPX2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Facial Emotion Detection Model using CNN\n",
        "Designing the CNN model for emotion detection.\n",
        "\n",
        "We are creating blocks using Conv2D layer, Batch-Normalization, Max-Pooling2D, Dropout, Flatten, and then stacking them together and at the end-use Dense Layer for output."
      ],
      "metadata": {
        "id": "s22pOW3jwHp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# Layer 1\n",
        "model.add(Conv2D(32,(3,3),padding='same', input_shape=(48, 48, 1), activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32,(3,3),padding='same', activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2))) #reduce the spatial dimensions of the output volume\n",
        "model.add(Dropout(0.25))\n",
        "# Layer 2\n",
        "model.add(Conv2D(64,(3,3),padding='same', activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64,(3,3),padding='same', activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "# Layer 3\n",
        "model.add(Conv2D(128,(3,3),padding='same', activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128,(3,3),padding='same', activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "# Layer 5\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "# Layer 6\n",
        "model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "# Layer 7\n",
        "model.add(Dense(7, activation='softmax', kernel_initializer='he_uniform'))\n",
        "\n",
        "summary = model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer = 'adam', \n",
        "    loss = 'sparse_categorical_crossentropy', # multi class classification problem\n",
        "    metrics = ['accuracy'])\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4lSOtf2A5O8",
        "outputId": "a17b876e-09e4-4c51-b8bb-1f0da1cd09f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 48, 48, 32)        320       \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 48, 48, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 48, 48, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 48, 48, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 24, 24, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 24, 24, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 24, 24, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 24, 24, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 12, 12, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 12, 12, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 12, 12, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 12, 12, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 12, 12, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 6, 6, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 6, 6, 128)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                294976    \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 7)                 455       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 588,327\n",
            "Trainable params: 587,175\n",
            "Non-trainable params: 1,152\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = Sequential()\n",
        "# # Layer 1\n",
        "# model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_uniform', input_shape=(48, 48, 1), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_uniform', activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2,2))) #reduce the spatial dimensions of the output volume\n",
        "# model.add(Dropout(0.25))\n",
        "# # Layer 2\n",
        "# model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_uniform', activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_uniform', activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# model.add(Dropout(0.25))\n",
        "# # Layer 3\n",
        "# model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_uniform', activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_uniform', activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# model.add(Dropout(0.25))\n",
        "# Layer 4\n",
        "# model.add(Conv2D(256,(3,3),padding='same', kernel_initializer='he_uniform', activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_uniform', activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# model.add(Dropout(0.2))\n",
        "# # Layer 5\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(64,kernel_initializer='he_uniform', activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.5))\n",
        "# # Layer 6\n",
        "# model.add(Dense(64,kernel_initializer='he_uniform', activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(Dropout(0.5))\n",
        "# # Layer 7\n",
        "# model.add(Dense(7, kernel_initializer='he_uniform', activation='softmax'))\n",
        "\n",
        "# summary = model.summary()\n",
        "# opt = Adagrad(lr=0.001, epsilon=1e-06)\n",
        "\n",
        "# model.compile(\n",
        "#     optimizer = opt, \n",
        "#     loss = 'categorical_crossentropy', # multi class classification problem\n",
        "#     metrics = ['accuracy'])\n",
        "\n",
        "# print(summary)\n",
        "\n",
        "# # source : https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n",
        "# # source : https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy"
      ],
      "metadata": {
        "id": "KQrCacVXUGMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"./data/weights.h5\")"
      ],
      "metadata": {
        "id": "h4hsJRgn17Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting tho model with training and testing data"
      ],
      "metadata": {
        "id": "agxQQxcE7Qi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint('./data/checkpoint.h5',\n",
        "                             monitor='val_accuracy',\n",
        "                             mode='max',\n",
        "                             save_best_only=True,\n",
        "                             verbose=1)\n",
        "earlystop = EarlyStopping(monitor='val_accuracy',\n",
        "                          min_delta=0,\n",
        "                          patience=3,\n",
        "                          verbose=1,\n",
        "                          restore_best_weights=True\n",
        "                          )\n",
        "reduce_learning_rate = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.5,\n",
        "                              patience=3,\n",
        "                              verbose=1,\n",
        "                              min_delta=0.0001)\n",
        "\n",
        "callbacks_list = [earlystop,checkpoint,reduce_learning_rate]"
      ],
      "metadata": {
        "id": "bqTyKizT39c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=32, epochs=50, callbacks=callbacks_list, validation_split=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k03sr3hm9NdU",
        "outputId": "4208b7b7-6346-458d-ab02-79695db743b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 2.0996 - accuracy: 0.2370\n",
            "Epoch 00001: val_accuracy improved from inf to 0.35214, saving model to ./data/checkpoint.h5\n",
            "808/808 [==============================] - 325s 401ms/step - loss: 2.0996 - accuracy: 0.2370 - val_loss: 1.6240 - val_accuracy: 0.3521 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.6050 - accuracy: 0.3736\n",
            "Epoch 00002: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 323s 399ms/step - loss: 1.6050 - accuracy: 0.3736 - val_loss: 1.3889 - val_accuracy: 0.4598 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.4491 - accuracy: 0.4443\n",
            "Epoch 00003: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 324s 401ms/step - loss: 1.4491 - accuracy: 0.4443 - val_loss: 1.2612 - val_accuracy: 0.5225 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.3649 - accuracy: 0.4768\n",
            "Epoch 00004: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 324s 400ms/step - loss: 1.3649 - accuracy: 0.4768 - val_loss: 1.1941 - val_accuracy: 0.5462 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.3072 - accuracy: 0.5040\n",
            "Epoch 00005: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 323s 400ms/step - loss: 1.3072 - accuracy: 0.5040 - val_loss: 1.1386 - val_accuracy: 0.5712 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.2589 - accuracy: 0.5232\n",
            "Epoch 00006: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 323s 399ms/step - loss: 1.2589 - accuracy: 0.5232 - val_loss: 1.1263 - val_accuracy: 0.5737 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.2184 - accuracy: 0.5430\n",
            "Epoch 00007: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 328s 406ms/step - loss: 1.2184 - accuracy: 0.5430 - val_loss: 1.0944 - val_accuracy: 0.5845 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.1771 - accuracy: 0.5658\n",
            "Epoch 00008: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 324s 401ms/step - loss: 1.1771 - accuracy: 0.5658 - val_loss: 1.0930 - val_accuracy: 0.5866 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.1324 - accuracy: 0.5830\n",
            "Epoch 00009: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 324s 401ms/step - loss: 1.1324 - accuracy: 0.5830 - val_loss: 1.0307 - val_accuracy: 0.6127 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.1067 - accuracy: 0.5935\n",
            "Epoch 00010: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 328s 405ms/step - loss: 1.1067 - accuracy: 0.5935 - val_loss: 1.0218 - val_accuracy: 0.6155 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.0608 - accuracy: 0.6151\n",
            "Epoch 00011: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 325s 403ms/step - loss: 1.0608 - accuracy: 0.6151 - val_loss: 1.0246 - val_accuracy: 0.6162 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.0396 - accuracy: 0.6229\n",
            "Epoch 00012: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 325s 402ms/step - loss: 1.0396 - accuracy: 0.6229 - val_loss: 0.9934 - val_accuracy: 0.6290 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.0003 - accuracy: 0.6418\n",
            "Epoch 00013: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 328s 405ms/step - loss: 1.0003 - accuracy: 0.6418 - val_loss: 1.0276 - val_accuracy: 0.6186 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 0.9836 - accuracy: 0.6444\n",
            "Epoch 00014: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 325s 403ms/step - loss: 0.9836 - accuracy: 0.6444 - val_loss: 1.0044 - val_accuracy: 0.6169 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 0.9505 - accuracy: 0.6571\n",
            "Epoch 00015: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 326s 404ms/step - loss: 0.9505 - accuracy: 0.6571 - val_loss: 0.9879 - val_accuracy: 0.6304 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 0.9139 - accuracy: 0.6727\n",
            "Epoch 00016: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 324s 402ms/step - loss: 0.9139 - accuracy: 0.6727 - val_loss: 0.9971 - val_accuracy: 0.6284 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 0.8855 - accuracy: 0.6841\n",
            "Epoch 00017: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 324s 401ms/step - loss: 0.8855 - accuracy: 0.6841 - val_loss: 0.9984 - val_accuracy: 0.6277 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 0.8642 - accuracy: 0.6893\n",
            "Epoch 00018: val_accuracy did not improve from 0.35214\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "808/808 [==============================] - 324s 401ms/step - loss: 0.8642 - accuracy: 0.6893 - val_loss: 1.0084 - val_accuracy: 0.6357 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 0.7965 - accuracy: 0.7150\n",
            "Epoch 00019: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 323s 400ms/step - loss: 0.7965 - accuracy: 0.7150 - val_loss: 0.9818 - val_accuracy: 0.6486 - lr: 5.0000e-04\n",
            "Epoch 20/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 0.7507 - accuracy: 0.7357\n",
            "Epoch 00020: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 324s 401ms/step - loss: 0.7507 - accuracy: 0.7357 - val_loss: 0.9914 - val_accuracy: 0.6503 - lr: 5.0000e-04\n",
            "Epoch 21/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 0.7280 - accuracy: 0.7460\n",
            "Epoch 00021: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 325s 403ms/step - loss: 0.7280 - accuracy: 0.7460 - val_loss: 1.0121 - val_accuracy: 0.6499 - lr: 5.0000e-04\n",
            "Epoch 22/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 0.7007 - accuracy: 0.7534\n",
            "Epoch 00022: val_accuracy did not improve from 0.35214\n",
            "\n",
            "Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "808/808 [==============================] - 332s 411ms/step - loss: 0.7007 - accuracy: 0.7534 - val_loss: 1.0209 - val_accuracy: 0.6440 - lr: 5.0000e-04\n",
            "Epoch 23/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 0.6540 - accuracy: 0.7703Restoring model weights from the end of the best epoch: 20.\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.35214\n",
            "808/808 [==============================] - 332s 411ms/step - loss: 0.6540 - accuracy: 0.7703 - val_loss: 1.0332 - val_accuracy: 0.6493 - lr: 2.5000e-04\n",
            "Epoch 00023: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcdbdcabdd0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving and evaluating the model"
      ],
      "metadata": {
        "id": "6gS_Uk50FQeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('./data/output')\n",
        "model.load_weights('./data/checkpoint.h5')\n",
        "eval = model.evaluate(x_test, y_test, verbose=1)\n",
        "# TODO : summarize_diagnosis to pyplot as in : https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/"
      ],
      "metadata": {
        "id": "-qwmrKxTFNKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd4a3454-9a95-4f80-ce68-cf44713a2edb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ./data/output/assets\n",
            "113/113 [==============================] - 11s 101ms/step - loss: 1.6345 - accuracy: 0.3550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('dark_background')\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.ylabel('Accuracy', fontsize=16)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "74SsU9Q--G18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autre modele test\n",
        "En changeant plusieurs parametres. Mon modèle s'appelle Heidi (Klum)."
      ],
      "metadata": {
        "id": "zSO3Nf8ZHqr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Heidi = Sequential()\n",
        "# Layer 1\n",
        "Heidi.add(Conv2D(32,(3,3),padding='same', input_shape=(48, 48, 1), activation='relu'))\n",
        "Heidi.add(BatchNormalization())\n",
        "Heidi.add(Conv2D(32,(3,3),padding='same', activation='relu'))\n",
        "Heidi.add(BatchNormalization())\n",
        "Heidi.add(MaxPooling2D(pool_size=(2,2))) #reduce the spatial dimensions of the output volume\n",
        "Heidi.add(Dropout(0.25))\n",
        "# Layer 2\n",
        "Heidi.add(Conv2D(64,(3,3),padding='same', activation='relu'))\n",
        "Heidi.add(BatchNormalization())\n",
        "Heidi.add(Conv2D(64,(3,3),padding='same', activation='relu'))\n",
        "Heidi.add(BatchNormalization())\n",
        "Heidi.add(MaxPooling2D(pool_size=(2,2)))\n",
        "Heidi.add(Dropout(0.25))\n",
        "# Layer 3\n",
        "Heidi.add(Conv2D(128,(3,3),padding='same', activation='relu'))\n",
        "Heidi.add(BatchNormalization())\n",
        "Heidi.add(Conv2D(128,(3,3),padding='same', activation='relu'))\n",
        "Heidi.add(BatchNormalization())\n",
        "Heidi.add(MaxPooling2D(pool_size=(2,2)))\n",
        "Heidi.add(Dropout(0.25))\n",
        "# Layer 4\n",
        "Heidi.add(Conv2D(256,(3,3),padding='same', activation='relu'))\n",
        "Heidi.add(BatchNormalization())\n",
        "Heidi.add(Conv2D(256,(3,3),padding='same', activation='relu'))\n",
        "Heidi.add(BatchNormalization())\n",
        "Heidi.add(MaxPooling2D(pool_size=(2,2)))\n",
        "Heidi.add(Dropout(0.2))\n",
        "# Layer 5\n",
        "Heidi.add(Flatten())\n",
        "Heidi.add(Dense(64, activation='relu'))\n",
        "Heidi.add(BatchNormalization())\n",
        "Heidi.add(Dropout(0.5))\n",
        "# Layer 6\n",
        "Heidi.add(Dense(64, activation='relu'))\n",
        "Heidi.add(BatchNormalization())\n",
        "Heidi.add(Dropout(0.5))\n",
        "# Layer 7\n",
        "Heidi.add(Dense(7, activation='softmax'))\n",
        "\n",
        "summary = Heidi.summary()\n",
        "opt = SGD(learning_rate=0.001)\n",
        "\n",
        "Heidi.compile(\n",
        "    optimizer = opt, \n",
        "    loss = 'sparse_categorical_crossentropy', # multi class classification problem : y_train and y_test have to be one-hot-encoded\n",
        "    metrics = ['accuracy'])\n",
        "\n",
        "print(summary)\n",
        "\n",
        "Heidi.save_weights(\"./data2/weights.h5\")\n",
        "\n",
        "checkpoint2 = ModelCheckpoint('./data2/checkpoint.h5',\n",
        "                             monitor='val_accuracy',\n",
        "                             mode='max',\n",
        "                             save_best_only=True,\n",
        "                              save_weights_only=True,\n",
        "                             verbose=1)\n",
        "earlystop2 = EarlyStopping(monitor='val_accuracy',\n",
        "                          min_delta=0,\n",
        "                          patience=3,\n",
        "                          verbose=1,\n",
        "                          restore_best_weights=True\n",
        "                          )\n",
        "reduce_learning_rate2 = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.7,\n",
        "                              patience=3,\n",
        "                              verbose=1)\n",
        "\n",
        "callbacks_list2 = [earlystop2,checkpoint2,reduce_learning_rate2]"
      ],
      "metadata": {
        "id": "eKa8u6hYHwQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d3bb53-5d5e-4735-b004-6d4436c3d77e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_55 (Conv2D)          (None, 48, 48, 32)        320       \n",
            "                                                                 \n",
            " batch_normalization_70 (Bat  (None, 48, 48, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_56 (Conv2D)          (None, 48, 48, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_71 (Bat  (None, 48, 48, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_27 (MaxPoolin  (None, 24, 24, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_43 (Dropout)        (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " conv2d_57 (Conv2D)          (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_72 (Bat  (None, 24, 24, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_58 (Conv2D)          (None, 24, 24, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_73 (Bat  (None, 24, 24, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_28 (MaxPoolin  (None, 12, 12, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_44 (Dropout)        (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " conv2d_59 (Conv2D)          (None, 12, 12, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_74 (Bat  (None, 12, 12, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_60 (Conv2D)          (None, 12, 12, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_75 (Bat  (None, 12, 12, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_29 (MaxPoolin  (None, 6, 6, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_45 (Dropout)        (None, 6, 6, 128)         0         \n",
            "                                                                 \n",
            " conv2d_61 (Conv2D)          (None, 6, 6, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_76 (Bat  (None, 6, 6, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_62 (Conv2D)          (None, 6, 6, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_77 (Bat  (None, 6, 6, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_30 (MaxPoolin  (None, 3, 3, 256)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_46 (Dropout)        (None, 3, 3, 256)         0         \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 2304)              0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 64)                147520    \n",
            "                                                                 \n",
            " batch_normalization_78 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_47 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " batch_normalization_79 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_48 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 7)                 455       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,328,167\n",
            "Trainable params: 1,325,991\n",
            "Non-trainable params: 2,176\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training, saving and evaluating Heidi"
      ],
      "metadata": {
        "id": "p0jSiQ3Z4NHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Heidi.fit(x_train, y_train, batch_size=32, epochs=50, callbacks=callbacks_list2, validation_split=0.1)\n",
        "\n",
        "Heidi.save('./data2/output')\n",
        "Heidi.load_weights('./data2/checkpoint.h5')\n",
        "eval2 = Heidi.evaluate(x_test, y_test, verbose=1)\n",
        "print(f'Test set loss: {eval2[0]}\\nTest set accuracy: {eval2[1]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoPuvr9J4JgB",
        "outputId": "ac4d5196-8ba8-446e-e738-535b72b44444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 2.7402 - accuracy: 0.1658\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.25148, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 444s 548ms/step - loss: 2.7402 - accuracy: 0.1658 - val_loss: 1.8809 - val_accuracy: 0.2515 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 2.4303 - accuracy: 0.1836\n",
            "Epoch 00002: val_accuracy improved from 0.25148 to 0.26158, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 456s 565ms/step - loss: 2.4303 - accuracy: 0.1836 - val_loss: 1.8417 - val_accuracy: 0.2616 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 2.2487 - accuracy: 0.1980\n",
            "Epoch 00003: val_accuracy did not improve from 0.26158\n",
            "808/808 [==============================] - 438s 543ms/step - loss: 2.2487 - accuracy: 0.1980 - val_loss: 1.8360 - val_accuracy: 0.2602 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 2.1324 - accuracy: 0.2063\n",
            "Epoch 00004: val_accuracy improved from 0.26158 to 0.27342, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 439s 543ms/step - loss: 2.1324 - accuracy: 0.2063 - val_loss: 1.8037 - val_accuracy: 0.2734 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 2.0393 - accuracy: 0.2249\n",
            "Epoch 00005: val_accuracy did not improve from 0.27342\n",
            "808/808 [==============================] - 438s 542ms/step - loss: 2.0393 - accuracy: 0.2249 - val_loss: 1.7886 - val_accuracy: 0.2692 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.9874 - accuracy: 0.2294\n",
            "Epoch 00006: val_accuracy improved from 0.27342 to 0.27517, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 450s 557ms/step - loss: 1.9874 - accuracy: 0.2294 - val_loss: 1.7814 - val_accuracy: 0.2752 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.9377 - accuracy: 0.2409\n",
            "Epoch 00007: val_accuracy improved from 0.27517 to 0.28666, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 440s 545ms/step - loss: 1.9377 - accuracy: 0.2409 - val_loss: 1.7556 - val_accuracy: 0.2867 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.8943 - accuracy: 0.2486\n",
            "Epoch 00008: val_accuracy improved from 0.28666 to 0.29328, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 438s 542ms/step - loss: 1.8943 - accuracy: 0.2486 - val_loss: 1.7447 - val_accuracy: 0.2933 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.8621 - accuracy: 0.2581\n",
            "Epoch 00009: val_accuracy improved from 0.29328 to 0.29850, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 438s 542ms/step - loss: 1.8621 - accuracy: 0.2581 - val_loss: 1.7364 - val_accuracy: 0.2985 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.8274 - accuracy: 0.2696\n",
            "Epoch 00010: val_accuracy improved from 0.29850 to 0.30024, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 442s 546ms/step - loss: 1.8274 - accuracy: 0.2696 - val_loss: 1.7261 - val_accuracy: 0.3002 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.8083 - accuracy: 0.2754\n",
            "Epoch 00011: val_accuracy improved from 0.30024 to 0.30338, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 438s 542ms/step - loss: 1.8083 - accuracy: 0.2754 - val_loss: 1.7134 - val_accuracy: 0.3034 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.7908 - accuracy: 0.2795\n",
            "Epoch 00012: val_accuracy did not improve from 0.30338\n",
            "808/808 [==============================] - 446s 552ms/step - loss: 1.7908 - accuracy: 0.2795 - val_loss: 1.7090 - val_accuracy: 0.3030 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.7649 - accuracy: 0.2932\n",
            "Epoch 00013: val_accuracy improved from 0.30338 to 0.30651, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 446s 552ms/step - loss: 1.7649 - accuracy: 0.2932 - val_loss: 1.7001 - val_accuracy: 0.3065 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.7510 - accuracy: 0.2990\n",
            "Epoch 00014: val_accuracy improved from 0.30651 to 0.30721, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 445s 551ms/step - loss: 1.7510 - accuracy: 0.2990 - val_loss: 1.6892 - val_accuracy: 0.3072 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.7293 - accuracy: 0.3082\n",
            "Epoch 00015: val_accuracy improved from 0.30721 to 0.31836, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 444s 549ms/step - loss: 1.7293 - accuracy: 0.3082 - val_loss: 1.6794 - val_accuracy: 0.3184 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.7159 - accuracy: 0.3203\n",
            "Epoch 00016: val_accuracy did not improve from 0.31836\n",
            "808/808 [==============================] - 435s 538ms/step - loss: 1.7159 - accuracy: 0.3203 - val_loss: 1.6730 - val_accuracy: 0.3163 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.6974 - accuracy: 0.3243\n",
            "Epoch 00017: val_accuracy did not improve from 0.31836\n",
            "808/808 [==============================] - 433s 536ms/step - loss: 1.6974 - accuracy: 0.3243 - val_loss: 1.7085 - val_accuracy: 0.3110 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.6809 - accuracy: 0.3337\n",
            "Epoch 00018: val_accuracy improved from 0.31836 to 0.31975, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 433s 536ms/step - loss: 1.6809 - accuracy: 0.3337 - val_loss: 1.7062 - val_accuracy: 0.3197 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.6642 - accuracy: 0.3386\n",
            "Epoch 00019: val_accuracy did not improve from 0.31975\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
            "808/808 [==============================] - 444s 550ms/step - loss: 1.6642 - accuracy: 0.3386 - val_loss: 1.7681 - val_accuracy: 0.3072 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.6533 - accuracy: 0.3486\n",
            "Epoch 00020: val_accuracy improved from 0.31975 to 0.32323, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 442s 547ms/step - loss: 1.6533 - accuracy: 0.3486 - val_loss: 1.7295 - val_accuracy: 0.3232 - lr: 7.0000e-04\n",
            "Epoch 21/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.6427 - accuracy: 0.3549\n",
            "Epoch 00021: val_accuracy improved from 0.32323 to 0.32567, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 443s 548ms/step - loss: 1.6427 - accuracy: 0.3549 - val_loss: 1.7223 - val_accuracy: 0.3257 - lr: 7.0000e-04\n",
            "Epoch 22/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.6329 - accuracy: 0.3612\n",
            "Epoch 00022: val_accuracy improved from 0.32567 to 0.36085, saving model to ./data2/checkpoint.h5\n",
            "808/808 [==============================] - 451s 558ms/step - loss: 1.6329 - accuracy: 0.3612 - val_loss: 1.6136 - val_accuracy: 0.3608 - lr: 7.0000e-04\n",
            "Epoch 23/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.6245 - accuracy: 0.3658\n",
            "Epoch 00023: val_accuracy did not improve from 0.36085\n",
            "808/808 [==============================] - 444s 550ms/step - loss: 1.6245 - accuracy: 0.3658 - val_loss: 1.6436 - val_accuracy: 0.3480 - lr: 7.0000e-04\n",
            "Epoch 24/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.6106 - accuracy: 0.3714\n",
            "Epoch 00024: val_accuracy did not improve from 0.36085\n",
            "808/808 [==============================] - 444s 549ms/step - loss: 1.6106 - accuracy: 0.3714 - val_loss: 1.6716 - val_accuracy: 0.3452 - lr: 7.0000e-04\n",
            "Epoch 25/50\n",
            "808/808 [==============================] - ETA: 0s - loss: 1.6069 - accuracy: 0.3730Restoring model weights from the end of the best epoch: 22.\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.36085\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
            "808/808 [==============================] - 444s 550ms/step - loss: 1.6069 - accuracy: 0.3730 - val_loss: 1.6481 - val_accuracy: 0.3539 - lr: 7.0000e-04\n",
            "Epoch 00025: early stopping\n",
            "INFO:tensorflow:Assets written to: ./data2/output/assets\n",
            "113/113 [==============================] - 14s 127ms/step - loss: 1.6185 - accuracy: 0.3650\n",
            "Test set loss: 1.6184970140457153\n",
            "Test set accuracy: 0.36500418186187744\n"
          ]
        }
      ]
    }
  ]
}